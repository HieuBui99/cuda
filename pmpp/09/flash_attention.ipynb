{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdiv(a,b): return (a + b - 1) // b\n",
    "assert cdiv(10,2)==5\n",
    "assert cdiv(10,3)==4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 64\n",
    "d = 128\n",
    "\n",
    "Q = torch.randn(N, d)\n",
    "K = torch.randn(N, d)\n",
    "V = torch.randn(N, d)\n",
    "Bc, Br = 16, 16\n",
    "Tr = cdiv(N, Br)\n",
    "Tc = cdiv(N, Bc)\n",
    "\n",
    "O = torch.zeros(N, d)\n",
    "L = torch.zeros(N, 1)\n",
    "M = torch.empty(N, 1)\n",
    "M = M.fill_(float('-inf'))\n",
    "\n",
    "scale = 1.0 / (d ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(Tc):\n",
    "    Kj = K[j*Bc:(j+1)*Bc]\n",
    "    Vj = V[j*Bc:(j+1)*Bc]\n",
    "    for i in range(Tr):\n",
    "        Qi = Q[i*Br:(i+1)*Br]\n",
    "        Oi = O[i*Br:(i+1)*Br]\n",
    "        Li = L[i*Br:(i+1)*Br]\n",
    "        Mi = M[i*Br:(i+1)*Br]\n",
    "        S = Qi @ Kj.T * scale\n",
    "        row_max = S.max(dim=1, keepdim=True)[0]\n",
    "        P = torch.exp(S - row_max) # (Br, Bc)\n",
    "        row_sum = P.sum(dim=1, keepdim=True) # (Br, 1)\n",
    "        Mi_new = torch.maximum(Mi, row_max) # (Br, 1)\n",
    "        Li_new = torch.exp(Mi - Mi_new) * Li + torch.exp(row_max - Mi_new) * row_sum # (Br, 1)\n",
    "\n",
    "        Oi = torch.inverse(torch.diag(Li_new.squeeze(1))) @ (torch.diag(Li.squeeze(1)) @ torch.exp(Mi-Mi_new) * Oi + torch.exp(row_max - Mi_new) * P @ Vj)\n",
    "        O[i*Br:(i+1)*Br] = Oi\n",
    "        L[i*Br:(i+1)*Br] = Li_new \n",
    "        M[i*Br:(i+1)*Br] = Mi_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        ...,\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.exp(Mi-Mi_new) * Oi) == torch.diag(torch.exp(Mi-Mi_new).squeeze(1)) @ Oi ## equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (16) must match the size of tensor b (128) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mMi_new\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mOi\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (16) must match the size of tensor b (128) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "torch.diag(torch.exp(Mi-Mi_new).squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [ True, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False,  True],\n",
       "        [ True, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False,  True, False]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.scaled_dot_product_attention(Q, K, V, None, scale=scale) == O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "NUM_HEADS = 8\n",
    "SEQ_LEN = 128\n",
    "HEAD_DIM = 32\n",
    "scale = 1.0 / (HEAD_DIM ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attn_fwd_inner(\n",
    "    O_block,\n",
    "    l_i,\n",
    "    m_i,\n",
    "    Q_block,\n",
    "    K_block_ptr,\n",
    "    V_block_ptr,\n",
    "    block_index_q,\n",
    "    softmax_scale,\n",
    "    BLOCK_SIZE_Q: tl.constexpr,\n",
    "    BLOCK_SIZE_KV: tl.constexpr,\n",
    "    STAGE: tl.constexpr,\n",
    "    offs_q: tl.constexpr,\n",
    "    offs_kv: tl.constexpr,\n",
    "    SEQ_LEN: tl.constexpr,\n",
    "):\n",
    "    # range of values handled by this stage\n",
    "    if STAGE == 1:\n",
    "        # From 0 to the left of the diagonal\n",
    "        lo, hi = 0, block_index_q * BLOCK_SIZE_Q\n",
    "    elif STAGE == 2:\n",
    "        # Used only for the block in which there is transition between non-masked and masked keys\n",
    "        lo, hi = block_index_q * BLOCK_SIZE_Q, (block_index_q + 1) * BLOCK_SIZE_Q\n",
    "        lo = tl.multiple_of(lo, BLOCK_SIZE_Q)\n",
    "    else:\n",
    "        # Only used for non-causal attention\n",
    "        lo, hi = 0, SEQ_LEN\n",
    "\n",
    "    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n",
    "    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n",
    "\n",
    "    # loop over k, v and update accumulator\n",
    "    for start_kv in range(lo, hi, BLOCK_SIZE_KV):\n",
    "        # Just let the compiler know that start_n is a multiple of BLOCK_N, so the compiler can do optimizations\n",
    "        start_kv = tl.multiple_of(start_kv, BLOCK_SIZE_KV)\n",
    "\n",
    "        # -- compute qk ----\n",
    "        K_block = tl.load(K_block_ptr)\n",
    "        QK_block = tl.dot(Q_block, K_block)\n",
    "\n",
    "        if STAGE == 2:\n",
    "            mask = offs_q[:, None] >= (start_kv + offs_kv[None, :])\n",
    "            QK_block = QK_block * softmax_scale + tl.where(mask, 0, -1.0e6)\n",
    "            m_ij = tl.maximum(m_i, tl.max(QK_block, 1))\n",
    "            QK_block -= m_ij[:, None]\n",
    "        else:\n",
    "            # Compute the maximum value of qk or keep the old max value\n",
    "            m_ij = tl.maximum(m_i, tl.max(QK_block, 1) * softmax_scale)\n",
    "            QK_block = QK_block * softmax_scale - m_ij[:, None]\n",
    "\n",
    "        # Compute the exponential of each dot product, so now we are computing exp(qk_ij - m_ij)\n",
    "        P_block = tl.math.exp(QK_block)\n",
    "        # Compute the sum by rows of the attention scores\n",
    "        l_ij = tl.sum(P_block, 1)\n",
    "\n",
    "        # This is the correction factor for the previous l_i\n",
    "        alpha = tl.math.exp(m_i - m_ij)\n",
    "        # Apply the correction factor to the previous l_i and add the new l_ij\n",
    "        l_i = l_i * alpha + l_ij\n",
    "\n",
    "        V_block = tl.load(V_block_ptr)\n",
    "        # P_block = P_block.to(tl.float16)\n",
    "        # This computes the following: O_new = P x V + O_old * alpha\n",
    "        O_block = O_block * alpha[:, None]\n",
    "        O_block = tl.dot(P_block, V_block, O_block)\n",
    "\n",
    "        m_i = m_ij\n",
    "\n",
    "        # Move to the next block of K and V\n",
    "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_SIZE_KV, 0))\n",
    "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_SIZE_KV))\n",
    "    return O_block, l_i, m_i\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    [\n",
    "        triton.Config(\n",
    "            {\"BLOCK_SIZE_Q\": BLOCK_SIZE_Q, \"BLOCK_SIZE_KV\": BLOCK_SIZE_KV},\n",
    "            num_stages=num_stages,\n",
    "            num_warps=num_warps,\n",
    "        )\n",
    "        for BLOCK_SIZE_Q in [64, 128]\n",
    "        for BLOCK_SIZE_KV in [32, 64]\n",
    "        for num_stages in ([3, 4, 7])\n",
    "        for num_warps in [2, 4]\n",
    "    ],\n",
    "    key=[\"SEQ_LEN\", \"HEAD_DIM\"],\n",
    ")\n",
    "@triton.jit\n",
    "def dot_product_attention_kernel(\n",
    "    Q, K, V, O, M, scale,\n",
    "    stride_Q_batch, stride_Q_head, stride_Q_seq, stride_Q_dim,\n",
    "    stride_K_batch, stride_K_head, stride_K_seq, stride_K_dim,\n",
    "    stride_V_batch, stride_V_head, stride_V_seq, stride_V_dim,\n",
    "    stride_O_batch, stride_O_head, stride_O_seq, stride_O_dim,\n",
    "    BATCH_SIZE: tl.constexpr, NUM_HEADS: tl.constexpr, SEQ_LEN: tl.constexpr, HEAD_DIM:tl.constexpr, BLOCK_SIZE_Q: tl.constexpr, BLOCK_SIZE_KV: tl.constexpr, stage: tl.constexpr\n",
    "):\n",
    "    \n",
    "    block_index_q = tl.program_id(0)\n",
    "    index_batch_head = tl.program_id(1)\n",
    "\n",
    "    index_batch = index_batch_head // NUM_HEADS\n",
    "    index_head = index_batch_head % NUM_HEADS\n",
    "\n",
    "    #Q[index_head, index_batch, :, :]\n",
    "    qkv_offset = index_batch.to(tl.int64) * stride_Q_batch + index_head.to(tl.int64) * stride_Q_head\n",
    "    \n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base=Q+qkv_offset, #Q[index_head, index_batch, block_index_q*BLOCK_SIZE_Q:, :]\n",
    "        shape=(SEQ_LEN, HEAD_DIM),\n",
    "        strides=(stride_Q_seq, stride_Q_dim),\n",
    "        offsets=(block_index_q * BLOCK_SIZE_Q, 0),\n",
    "        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),\n",
    "        order=(1, 0) # triton stuff, ignore cause idk what it does\n",
    "    )\n",
    "\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base=V+qkv_offset, #V[index_head, index_batch, :, :]\n",
    "        shape=(SEQ_LEN, HEAD_DIM),\n",
    "        strides=(stride_V_seq, stride_V_dim),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_SIZE_KV, HEAD_DIM),\n",
    "        order=(1, 0) # triton stuff, ignore cause idk what it does\n",
    "    )\n",
    "\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base=K+qkv_offset,\n",
    "        shape=(HEAD_DIM, SEQ_LEN),\n",
    "        strides=(stride_K_dim, stride_K_seq),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(HEAD_DIM, BLOCK_SIZE_KV),\n",
    "        order=(0, 1) # triton stuff, ignore cause idk what it does\n",
    "    )\n",
    "\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        base=O+qkv_offset,\n",
    "        shape=(SEQ_LEN, HEAD_DIM),\n",
    "        strides=(stride_O_seq, stride_O_dim),\n",
    "        offsets=(block_index_q * BLOCK_SIZE_Q, 0),\n",
    "        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),\n",
    "        order=(1, 0) # triton stuff, ignore cause idk what it does\n",
    "    )\n",
    "\n",
    "    offset_q = block_index_q * BLOCK_SIZE_Q + tl.arange(0, BLOCK_SIZE_Q)\n",
    "    offset_kv = tl.arange(0, BLOCK_SIZE_KV)\n",
    "\n",
    "    #running maximum for softmax\n",
    "    m_i = tl.zeros((BLOCK_SIZE_Q, ), dtype=tl.float32) - float('inf')\n",
    "    l_i = tl.zeros((BLOCK_SIZE_Q, ), dtype=tl.float32)\n",
    "    O_i = tl.zeros((BLOCK_SIZE_Q, HEAD_DIM), dtype=tl.float32)\n",
    "\n",
    "    Q_block = tl.load(Q_block_ptr) # (BLOCK_SIZE_Q, HEAD_DIM)\n",
    "\n",
    "    if stage == 1 or stage == 3:\n",
    "        O_i, l_i, m_i = _attn_fwd_inner(\n",
    "            O_i,\n",
    "            l_i,\n",
    "            m_i,\n",
    "            Q_block,\n",
    "            K_block_ptr,\n",
    "            V_block_ptr,\n",
    "            block_index_q,\n",
    "            scale,\n",
    "            BLOCK_SIZE_Q,\n",
    "            BLOCK_SIZE_KV,\n",
    "            4 - stage,\n",
    "            offset_q,\n",
    "            offset_kv,\n",
    "            SEQ_LEN\n",
    "        )\n",
    "        if stage == 3:\n",
    "            O_i, l_i, m_i = _attn_fwd_inner(\n",
    "                O_i,\n",
    "                l_i,\n",
    "                m_i,\n",
    "                Q_block,\n",
    "                K_block_ptr,\n",
    "                V_block_ptr,\n",
    "                block_index_q,\n",
    "                scale,\n",
    "                BLOCK_SIZE_Q,\n",
    "                BLOCK_SIZE_KV,\n",
    "                2,\n",
    "                offset_q,\n",
    "                offset_kv,\n",
    "                SEQ_LEN\n",
    "            )\n",
    "\n",
    "    m_i += tl.math.log(l_i)\n",
    "    \n",
    "    O_i = O_i / l_i[:, None]\n",
    "    m_ptrs = M + index_batch_head * SEQ_LEN + offset_q\n",
    "    tl.store(O_block_ptr, O_i)\n",
    "    tl.store(m_ptrs, m_i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(Q, K, V, scale, causal=False):\n",
    "    HEAD_DIM_Q, HEAD_DIM_K, HEAD_DIM_V = Q.shape[-1], K.shape[-1], V.shape[-1]\n",
    "\n",
    "    BATCH_SIZE, NUM_HEADS, SEQ_LEN, _ = Q.shape\n",
    "    assert HEAD_DIM_Q == HEAD_DIM_K == HEAD_DIM_V\n",
    "\n",
    "    O = torch.zeros_like(Q, device=Q.device)\n",
    "\n",
    "    M = torch.empty((BATCH_SIZE, NUM_HEADS, SEQ_LEN), dtype=torch.float32, device=Q.device)\n",
    "    grid = lambda args: (\n",
    "        triton.cdiv(SEQ_LEN, args[\"BLOCK_SIZE_Q\"]),\n",
    "        BATCH_SIZE * NUM_HEADS,\n",
    "        1\n",
    "    )\n",
    "\n",
    "    stage = 3 if causal else 1\n",
    "\n",
    "    dot_product_attention_kernel[grid](\n",
    "        Q=Q, K=K, V=V, O=O, M=M, scale=scale,\n",
    "        stride_Q_batch=Q.stride(0),\n",
    "        stride_Q_head=Q.stride(1),\n",
    "        stride_Q_seq=Q.stride(2),\n",
    "        stride_Q_dim=Q.stride(3),\n",
    "        stride_K_batch=K.stride(0),\n",
    "        stride_K_head=K.stride(1),\n",
    "        stride_K_seq=K.stride(2),\n",
    "        stride_K_dim=K.stride(3),\n",
    "        stride_V_batch=V.stride(0),\n",
    "        stride_V_head=V.stride(1),\n",
    "        stride_V_seq=V.stride(2),\n",
    "        stride_V_dim=V.stride(3),\n",
    "        stride_O_batch=O.stride(0),\n",
    "        stride_O_head=O.stride(1),\n",
    "        stride_O_seq=O.stride(2),\n",
    "        stride_O_dim=O.stride(3),\n",
    "        BATCH_SIZE=BATCH_SIZE,\n",
    "        NUM_HEADS=NUM_HEADS,\n",
    "        SEQ_LEN=SEQ_LEN,\n",
    "        HEAD_DIM=HEAD_DIM_K,\n",
    "        stage=stage,\n",
    "        BLOCK_SIZE_Q=64,\n",
    "        BLOCK_SIZE_KV=64\n",
    "    )\n",
    "\n",
    "    return O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.randn(BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM).cuda()\n",
    "K = torch.randn(BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM).cuda()\n",
    "V = torch.randn(BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM).cuda()\n",
    "scale = 1.0 / (HEAD_DIM ** 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.nn.functional.scaled_dot_product_attention(Q, K, V, None, scale=scale) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dot_product_attention(Q, K, V, scale)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
